# -*- coding: utf-8 -*-
"""CRISMIS GSoC Project Updated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d55KiCSzDbsYW9VxNak9phGlOijRgxU0

####DOWNLOADING THE DATA SET
"""

###Importing the necessary libraries for downloading data from web
import urllib.request
import re
import os
from os.path import isfile,join
from os import listdir

##This method first opens the url where all images from the given date are stored. Then it creates a directory and 
##downloads the images in the url. I used regular expressions to find the names of the images.
def get_image(day,year):
  data = urllib.request.urlopen("https://pdsimage2.wr.usgs.gov/archive/mess-e_v_h-mdis-2-edr-rawdata-v1.0/MSGRMDS_1001/DATA/"+str(year)+"_"+str(day)+"/")
  os.makedirs("Dataset"+str(year)+"-"+str(day))
  for line in data:
    found = re.search("href=\"(.*?\.IMG)",line.decode())
    if found:
      fullfilename = os.path.join("Dataset"+str(year)+"-"+str(day),str(found.group(1)))
      urllib.request.urlretrieve("https://pdsimage2.wr.usgs.gov/archive/mess-e_v_h-mdis-2-edr-rawdata-v1.0/MSGRMDS_1001/DATA/"+str(year)+"_"+str(day)+"/"+str(found.group(1)),fullfilename)

##I decided to download the images from dates that are known to contain cosmic ray artifacts. 
##Otherwise my dataset would probably be too imbalanced. This may take some time.
get_image(156,2011)
get_image(155,2011)
get_image(157,2011)
get_image(207,2011)
get_image(215,2014)

"""###VISUALIZING THE DATA SET"""

###Necessary imports for data processing
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

##This method displays the image with given path. Since IMG files have header before the image data, the method visualizes
##the last 1024x1024 or 512x512 part of the file. I noticed that the header contains the height and width of the images 
##after the line "OBJECT = IMAGE" and used these values to find the size of the image. For example, an header might be:
## .....
## OBJECT = IMAGE
## LINES              = 1024
## LINE_SAMPLES       = 1024
##
## SAMPLE_BITS        = 8 
def visualize(path):
  with open(path, "rb") as f:
      line = ""
      while line != b'OBJECT = IMAGE\r\n':
        line = f.readline()
      line = f.readline().decode()
      H = int(line.split()[-1])#LINES = ...
      line = f.readline().decode()#LINE_SAMPLES = ...
      W = int(line.split()[-1])
      f.readline()
      line = f.readline().decode()#SAMPLE_BITS = ... 
      C = int(int(line.split()[-1])/8)# (I divided this number by 8 to find number of channels)
      img = f.read()
      arr = list(img)
      arr = arr[-H*W*C:]#LAST H*W*C part
      arr = np.array(arr).reshape(H,W*C)
      plt.imshow(arr/255,cmap="gray")

visualize("/content/Dataset2011-207/EW0220137668G.IMG")

visualize("/content/Dataset2014-215/EN1049375684M.IMG")

"""###PROCESSING THE DATA SET"""

##This method uses the same approach to identify images in .IMG file, imgToNpy method saves the image as npy file
## and as .png file. I used the npy files to create a dataset from np arrays and I used png files to display the images
##to label the dataset. imgToNpy method also deletes the .IMG files since we don't need them any further.
def imgToNpy(path):
  for file in os.listdir(path):
    if isfile(join(path,file)):
      with open(path+"/"+file, "rb") as f:
        line = ""
        while line != b'OBJECT = IMAGE\r\n':
          line = f.readline()
        line = f.readline().decode()
        H = int(line.split()[-1])
        line = f.readline().decode()
        W = int(line.split()[-1])
        f.readline()
        line = f.readline().decode()
        C = int(int(line.split()[-1])/8)
        img = f.read()
        arr = list(img)
        arr = arr[-H*W*C:]
        arr = np.array(arr).reshape(H,W*C)
        arr = arr/255
        np.save(path+"/"+str(file), arr)
        plt.imsave(path+"/"+str(file)+".png", arr)
        os.remove(path+"/"+str(file))

##Applying the method for the sets I downloaded.
imgToNpy("Dataset2011-156")
imgToNpy("Dataset2011-155")
imgToNpy("Dataset2014-215")
imgToNpy("Dataset2011-157")
imgToNpy("Dataset2011-207")

##I used an annotation tool to easily label images. This import are necessary to use that annotation tool.
!pip install pigeon-jupyter
from pigeon import annotate
from IPython.display import display
from PIL import Image
import pickle

##This method uses the annotation tool to label all images in the given path.
def labelingWithAnnotation(path):
  data = [path+"/"+str(f) for f in listdir(path) if isfile(join(path, f)) and join(path, f).endswith('.png')]
  annotations = annotate(data, options=["No Artifact","Artifact"], display_fn=lambda filename: display(Image.open(filename).resize((500,500)).convert("LA")))
  return annotations

"""Please skip the next two cells, I used them to label my sets but I will provide you the pickle files with labels, so you don't need to run these two cells."""

labels = labelingWithAnnotation("Dataset2011-156")

##This part is to save labels as pickle files so that I can download the pickle files and use them without labeling the
##dataset again. Also, I labelled the data using .png files but I will use the npy arrays to create a dataset, so this
##part also fixes the labels by changing extension of the images to .npy
fixed = []
for i in labels:
    fixed.append((str(i[0][:-3]+"npy"),i[1]))
pickle.dump(fixed, open("Dataset2011-156.p", "wb" ) )

"""You can continue from the cell below. You can upload the pickle files to use them."""

##This part is to combine 5 dataset I labelled, it just brings the labels together using their pickle files.
import pickle
labelarr= []
labelarr = labelarr + pickle.load(open("Dataset2011-155.p","rb"))
labelarr = labelarr + pickle.load(open("Dataset2011-156.p","rb"))
labelarr = labelarr + pickle.load(open("Dataset2011-157.p","rb"))
labelarr = labelarr + pickle.load(open("Dataset2011-207.p","rb"))
labelarr = labelarr + pickle.load(open("Dataset2014-215.p","rb"))
print(len(labelarr))

##This part uses labelarr to create trainset and labelset. It loads the np arrays from the file names and puts the arrays
##into trainset and labels into labelset. I also resized 1024x1024 images to 512x512 because 1024x1024 images turned out
##to be too big for my CNN models to train.
from skimage.transform import resize
trainset = np.zeros((652,512,512))
labelset = np.zeros((652,1))
for i in range(len(labelarr)):
  arr = (np.load(labelarr[i][0][:-3]+"IMG.npy"))
  if arr.shape ==(1024,1024):
    arr = resize(arr,(512,512))
  trainset[i] = arr
  if labelarr[i][1] == "Artifact":
    labelset[i] = 1
  elif labelarr[i][1] == "No Artifact":
    labelset[i] = 0
trainset = np.array(trainset)
labelset = np.array(labelset)

##Let's see how many of the labels are 1, how many of them are 0.
print(sum(value == 1 for value in labelset),sum(value == 1 for value in labelset)/len(labelset))
print(sum(value == 0 for value in labelset),sum(value == 0 for value in labelset)/len(labelset))

"""Only 18% of the label set are positive examples, our data set is imbalanced. There are approaches such as Oversampling, Undersampling, class weighting, threshold modification and custom loss function implementation to deal with imbalanced data sets. I didn't use Oversampling and Undersampling since I didn't want to lose any information from the small dataset I labelled. I did use custom loss functions to increase F1 score but they didn't give good results. So I decided to use threshold modification and class weighting approaches"""

trainset[124].shape

##I just randomly displayed an image to check for errors.
plt.imshow(trainset[232],cmap="gray")

##I finally reshaped trainset so that it is in the format (m,h,w,c). I also used train_test_split to create test set.
from sklearn.model_selection import train_test_split
trainset = trainset.reshape(trainset.shape[0],trainset.shape[1],trainset.shape[2],1)
X_train, X_test, y_train, y_test = train_test_split(trainset, labelset, test_size=0.10, random_state=42)

"""###EVALUATION METRICS"""

##I implemented an evaluation function to check the performance of my models. This was necessary because our data set is
##imbalanced and accuracy is not an efficient metric for imbalanced sets. This method displays the Precision, Recall and
##F1 score of our data set, which are much more efficient metrics for imbalanced sets. I also implemented threshold
##modification in this method. Instead of labelling images 1 when the prediction is bigger than 0.5, I changed the 0.5 to
##the threshold value that gives the best F1 score.
def evaluate(x_test,y_test,threshold,model):
  yhat = model.predict(x_test)
  yhat_binary = []
  for i in yhat:
    if i > threshold:
      yhat_binary.append(1)
    else:
      yhat_binary.append(0)
  print(classification_report(y_test,yhat_binary))

##This method finds the best threshold value to maximize F1 score, it tries every value in [0.05,0.1,0.15...] and returns
##the value which gives the best F1 score for train set. Note that, I only found the best threshold value for train set
##and used it to evaluate test set. Choosing best threshold for test set would be overfitting to test set, which is not a
##good approach. 
def best_threshold(x_test,y_test,model):
  thresholds = [0.05*i for i in range(1,20)]
  max = 0
  maxhold = 0
  yhat = model.predict(x_test)
  for threshold in thresholds:
     yhat_binary = []
     for i in yhat:
      if i > threshold:
        yhat_binary.append(1)
      else:
        yhat_binary.append(0)
     f1 = f1_score(y_test, yhat_binary, average='macro')
     if f1 > max:
      max = f1
      maxhold = i
  return maxhold

"""###BASIC MODEL"""

###Import for CNN models
from keras.layers import Conv2D, BatchNormalization, Dropout, Input, Dense, Flatten, MaxPool2D, GlobalAveragePooling2D
from keras.models import Sequential
from keras.optimizers import Adam
from keras.applications.vgg16 import VGG16
from keras.models import Model
from keras import regularizers
import tensorflow as tf
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score

##I implemented a basic CNN model first, I added two Conv layers and 2 Dense layers, output is sigmoid function for 
##binary classification.
basicModel = Sequential()

#Layer1
basicModel.add(Conv2D(16,(5,5),activation='relu',padding='valid',kernel_initializer="glorot_normal"))
basicModel.add(MaxPool2D())
basicModel.add(Dropout(0.2))

basicModel.add(Conv2D(16,(5,5),activation='relu',padding='valid',kernel_initializer="glorot_normal"))
basicModel.add(MaxPool2D())

basicModel.add(Flatten())
basicModel.add(Dense(16,activation="relu"))
basicModel.add(Dense(1,activation="sigmoid"))

##I used Adam as optimizer, loss function is binary cross entropy
opt = Adam(lr=0.001)
basicModel.compile(optimizer=opt,
               loss= "binary_crossentropy",
               metrics=['accuracy'])

##I used validation split to notice overfitting, though with such a small data set, overfitting was inevitable.
basicModel.fit(X_train,y_train,batch_size=16,epochs=10,validation_split=0.1,shuffle=True)

threshold = best_threshold(X_train,y_train,basicModel)##Best threshold for trainset
evaluate(X_train,y_train,threshold,basicModel)##Performance of the model with this threshold.

evaluate(X_test,y_test,threshold,basicModel)##Performance of model on test set.

"""I got 0.82 F1 score in the train set and 0.72 F1 score in the test set. I think this results are good with as the model was very simple. Also, the model clearly overfits the train set.

###TRANSFER LEARNING MODEL WITH CLASS WEIGHTING
"""

###I used also a model with transfer learning from VGG16. I thought my model could use the shallow layers of VGG16.
###As I thought, this model gave better results.
base_model=VGG16(weights='imagenet',include_top=False) #imports the VGG16 model and discards the output layer.
x = base_model.output
x = GlobalAveragePooling2D()(x)
x=Dense(32,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.
preds=Dense(1,activation='sigmoid')(x) #final layer
transferModel = Model(inputs=base_model.input, outputs=preds)
for layer in transferModel.layers[:16]:## I froze the first 16 layers of the model, I left remaining layers to get better results.
    layer.trainable=False

transferModel.compile(optimizer=Adam(0.0001),loss="binary_crossentropy",metrics=['accuracy'])
transferModel.summary()

##In the fit function, I needed to change input a little bit. Since the model uses transfer learning, my inputs should
##be valid inputs for VGG16 model, but VGG16 model requires RGB inputs while my images are grayscale. So, I repeated 
##trainset 3 times in the channel axis to get valid inputs.
##Also, I used class weighting approach here to deal with my imbalanced dataset. I give the argument class_weight such
##that an image with label 1 is 7 times more important than an image with label 0. 7 here is a hyperparameter, I tried
##several values and 7 turned out to be the best. 
transferModel.fit(np.repeat(X_train,3,-1),y_train,batch_size=16,epochs=10,validation_split=0.05,class_weight={0:1,1:7},)

threshold = best_threshold(np.repeat(X_train,3,-1),y_train,transferModel)#best threshold value for the train set
evaluate(np.repeat(X_train,3,-1),y_train,threshold,transferModel)#train set scores

evaluate(np.repeat(X_test,3,-1),y_test,threshold,transferModel)#test set scores

"""Transfer learning model gave 0.90 F1 score for train set and 0.83 F1 score for test set. It is clearly better than the simple model, though it still overfits the train set.

To get even better results, I would definitely get a bigger data set, more images with cosmic ray artifacts would be even better as the data set would be more balanced. Also, I would try different values for class weighting, would try freezing less layers of the VGG16 with a bigger data set. I would also use regularization to prevent overfitting, in my case, regularization gave bad results and didn't do much about overfitting, probably because of the size of the data set, and I decided not to use regularization.

###PREDICTIONS
"""

### This method is to visualize the image with the given path and print the prediction of the model for that image.
### Best threshold value is also used in predictions.
def prediction(path):
  orig = resize(np.load(path),(512,512)) ##These preprocessing lines are same as before
  pred = orig.reshape(1,512,512,1)
  pred = np.repeat(pred,3,-1)
  print("Prediction of the model is : " + str(1 if transferModel.predict(pred)>threshold else 0))
  plt.figure(figsize = (6,6))
  plt.imshow(orig,cmap="gray")

prediction("/content/Dataset2011-156/EW0215719903G.IMG.npy")
###This image contains many clear cosmic ray artifacts and the model predicts it correctly. I also noticed that the
###image contains lots of bright spots, I don't think all of these spots are cosmic artifacts since not all of them
###looks like lines, some of them are probably distant stars.

prediction("/content/Dataset2011-156/EW0215720500G.IMG.npy")
###This image contains a clear cosmic ray artifact at the bottom, it also contains smaller artifacts. 
###The prediction of the model is accurate. There are also many bright points similar to the previous image.

prediction("/content/Dataset2011-155/EW0215633149G.IMG.npy")
###There are no visible cosmic ray artifacts in this image, although the prediction of the model is 1.

prediction("/content/Dataset2011-155/EW0215676440G.IMG.npy")
###There is a bright spot in this image, I don't think it is an cosmic ray artifact. The model predicts 1, mistakenly.

"""I thought that the model predicts the wrong class for the image above because of the faulty data set. Because most of the images with cosmic ray artifacts also contain many bright spots and this might have caused my model to learn to classify images according to bright spots instead of cosmic ray artifacts. I decided to test my idea using the heatmap approach from the paper "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization". These heatmaps show the important parts of the images that model uses to classify. I used the listed function in the source "https://stackoverflow.com/questions/56477999/the-activation-in-my-cnn-does-not-look-correct-or-is-the-heatmap-the-problem" to create heatmaps. The next two methods are from the website, I only modified the first method so that the method displays the original image and the heatmap side by side."""

import keras.backend as K
import cv2
from google.colab.patches import cv2_imshow
def plot_conv_heat_map(model_prediction, input_layer, conv_layer, image_data, image_path):
    # Get the gradient of the winner class with regard to the output of the (last) conv. layer
    grads = K.gradients(model_prediction, conv_layer.output)[0]
    pooled_grads = K.mean(grads, axis=(0, 1, 2))
 
    # Get values of pooled grads and model conv. layer output as Numpy arrays
    iterate = K.function([input_layer], [pooled_grads, conv_layer.output[0]])
    pooled_grads_value, conv_layer_output_value = iterate([image_data])
 
    # Multiply each channel in the feature-map array by "how important this channel is"
    for i in range(pooled_grads_value.shape[0]):
        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]
 
    # The channel-wise mean of the resulting feature map is the heatmap of the class activation
    heatmap = np.mean(conv_layer_output_value, axis=-1)
    heatmap = np.maximum(heatmap, 0)
    max_heat = np.max(heatmap)
    if max_heat == 0:
        print("Heatmap generation failed, can't divide by zero!")
        return
    heatmap /= max_heat
 
    # Show heatmap in original (conv. layer) size
    #plt.matshow(heatmap)
    #plt.show()
    #return
 
    # Load image via CV2
    img = cv2.imread(image_path)
 
    # Resize heatmap to original image size, normalize it, convert to RGB, apply color map
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = cv2.normalize(heatmap, heatmap, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)
    heatmap = cv2.applyColorMap(np.uint8(255 * (255 - heatmap)), cv2.COLORMAP_JET)
   
    # Crate final image (combo of original pic + heatmap)
    superimposed = heatmap * 0.5 + img * 0.5
    image = _deprocess_tensor_to_image(superimposed)
    r = 500.0 / image.shape[0]
    dim = (int(image.shape[1] * r), 500)
    resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)
    cv2_imshow(np.hstack((np.repeat(cv2.resize(cv2.imread(image_path,0),dim, interpolation = cv2.INTER_AREA).reshape(500,500,1),3,2),resized)))
    #cv2_imshow(resized)

    cv2.waitKey(0)
 
def _deprocess_tensor_to_image(tensor):
    # Normalize the tensor: centers on 0, ensures that std is 0.1
    tensor -= tensor.mean()
    tensor /= (tensor.std()) + 1e-5
    tensor *= 0.1
 
    # Clip to [0,1]
    tensor += 0.5
    tensor = np.clip(tensor, 0, 1)
 
    # Converts to an RGB array
    tensor *= 255
    tensor = np.clip(tensor, 0, 255).astype("uint8")
 
    return tensor

"""plot_heatmap method is just to apply the heatmap approach to any image with given path. I created heatmaps according to the layer "block4_conv3", I tried using deeper layers but their heatmaps weren't helpful, this may be due to the fact that deeper layers identify very complex features. I think using the layer "block4_conv3" is not a bad choice since it is not a shallow layer and its output also has a big impact on the final output."""

def plot_heatmap(path):
  image_path = path+".png"
  image_data = resize(np.load(path+".npy"),(512,512))
  image_data = image_data.reshape(1,512,512,1)
  image_data = np.repeat(image_data,3,-1)
  preds = transferModel.predict(image_data)
  model_prediction = transferModel.output[:, np.argmax(preds[0])]
  input_layer = transferModel.input
  conv_layer = transferModel.get_layer("block4_conv3")
  plot_conv_heat_map(model_prediction, input_layer, conv_layer, image_data, image_path)

plot_heatmap("/content/Dataset2011-156/EN0215763748M.IMG")
### The heatmap shows that the visible cosmic ray at the bottom right is important for the output, as it should. 
### The heatmap also has greenish regions that contain bright spots instead of visible cosmic rays.

plot_heatmap("/content/Dataset2011-156/EN0215720407M.IMG")
### The big cosmic ray artifact at the bottom left is recognized by the heatmap. Though the heatmap also shows another
### red region that contain only a bright spot not a cosmic ray artifact.

plot_heatmap("/content/Dataset2011-156/EN0215720403M.IMG")

plot_heatmap("/content/Dataset2011-156/EW0215727932I.IMG")

"""From the heatmaps above, I think the mistakes are probably due to my data set, images with cosmic ray artifacts often also contain bright spots and these bright spots are recognized by the model as the heatmaps show. Getting a bigger data set would probably solve this, as it solves almost everything. Also, a deep model without transfer learning might be a better model once the data set gets big enough, since transfer learning may not be that helpful for a large data set."""